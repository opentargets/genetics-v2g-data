{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Javierre 2016 data\n",
    "\n",
    "The Javierre interval data is spilt by cell lines in which the data was recoreded.This information needs to be considered when the data is processed. As there's no expectation for this data to be changed, the most time consuming steps are written here and the resulting data is saved as partitioned parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import dataframe\n",
    "from functools import reduce\n",
    "import os\n",
    "\n",
    "import pyspark.sql\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "spark_mem_limit = 10\n",
    "\n",
    "spark_conf = (\n",
    "    SparkConf()\n",
    "    .set('spark.driver.memory', f'{spark_mem_limit}g')\n",
    "    .set('spark.executor.memory', f'{spark_mem_limit}g')\n",
    "    .set('spark.driver.maxResultSize', '0')\n",
    "    .set('spark.debug.maxToStringFields', '2000')\n",
    "    .set('spark.sql.execution.arrow.maxRecordsPerBatch', '500000')\n",
    "    .set('spark.ui.showConsoleProgress', 'false')\n",
    ")\n",
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder.config(conf=spark_conf)\n",
    "    .master('local[*]')\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "data_folder = 'gs://genetics-portal-input/v2g_input/javierre2016/'\n",
    "\n",
    "def read_data(input_directory: str, file_name: str) -> pd.DataFrame:\n",
    "    '''This function reads Javierre 2016 datasets\n",
    "\n",
    "    A compressed tsv file is read based on the provided file name and path.\n",
    "    The file name is read and parsed as bio_feature (later normalized as cell line.)\n",
    "    The function doesn't care if the files are in gs:// location or local.\n",
    "\n",
    "    Columns in the returned dataframe:\n",
    "    - chrom: chromosome for the first interval\n",
    "    - start: start position for the first interval\n",
    "    - end: end position for the first interval\n",
    "    - name: name of the second interval + score\n",
    "    - score: some sort of score... I don't exactly know.\n",
    "    - annotation: again, not sure what it is.\n",
    "    - bio_feature: raw cell type from the file name.\n",
    "\n",
    "    Params:\n",
    "        input_directory: str - path to the input directory\n",
    "        file_name: str - name of the file to read\n",
    "    Returns:\n",
    "        pyspak.dataFrame - dataframe with the data\n",
    "    '''\n",
    "    # logging.info(f'Reading file: {file_name}')\n",
    "    print(f'Reading file: {file_name}')\n",
    "    return spark.createDataFrame(\n",
    "        pd.read_csv(input_directory + file_name, sep='\\t', header=None)\n",
    "        .rename(columns={0: 'chrom', 1: 'start', 2: 'end', 3: 'name', 4: 'score', 5: 'annotation'})\n",
    "        .astype({'chrom': 'string', 'start': 'int', 'end': 'int', 'score': 'float'})\n",
    "        .assign(bio_feature=file_name.split('.')[0])\n",
    "    ).persist()\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"chrom\", T.StringType(), True),\n",
    "    T.StructField(\"start\", T.IntegerType(), True),\n",
    "    T.StructField(\"end\", T.IntegerType(), True),\n",
    "    T.StructField(\"name\", T.StringType(), True),\n",
    "    T.StructField(\"score\", T.StringType(), True),\n",
    "    T.StructField(\"annotation\", T.StringType(), True),\n",
    "    T.StructField(\"bio_feature\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "if data_folder.startswith('gs://'):\n",
    "    file_list = [x for x in gcsfs.GCSFileSystem().ls(data_folder) if x.endswith('.txt.gz')]\n",
    "else:\n",
    "    file_list = [x for x in os.listdir(data_folder) if x.endswith('.txt.gz')]\n",
    "\n",
    "df = reduce(lambda x, y: x.union(read_data(data_folder, y)), file_list, spark.createDataFrame(data=[], schema=schema))\n",
    "(\n",
    "    df\n",
    "    .orderBy('chrom', 'start')\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .parquet('gs://genetics-portal-input/v2g_input/javierre_2016_preprocessed.parquet')\n",
    ")\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
